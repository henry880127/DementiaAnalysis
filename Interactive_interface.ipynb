{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Specify the path to the pickle file\n",
    "data_file_path = '../Dataset/Dementia_paper_dataset_data.pkl'\n",
    "info_file_path = '../Dataset/Dementia_paper_dataset_info.pkl'\n",
    "\n",
    "# Open the pickle file in read mode\n",
    "with open(data_file_path, 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    data = pd.read_pickle(file)\n",
    "\n",
    "# Open the pickle file in read mode\n",
    "with open(info_file_path, 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    info = pd.read_pickle(file)\n",
    "\n",
    "# Display the first few rows of the data\n",
    "data['train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the file path for the Excel file\n",
    "excel_file_path = '../Dataset/Dementia_paper_dataset_data.xlsx'\n",
    "\n",
    "# Create an ExcelWriter object\n",
    "writer = pd.ExcelWriter(excel_file_path, engine='openpyxl')\n",
    "\n",
    "# Loop through the dictionary items and save each dataframe as a separate sheet in the Excel file\n",
    "for key, value in info.items():\n",
    "    if key != 'Info':\n",
    "        # Write the dataframe to the Excel file\n",
    "        value.to_excel(writer, sheet_name=key, index=False)\n",
    "\n",
    "# Save the Excel file\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HC: 0, MCI: 1, Dementia: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Dataframe to numpy array in each dictionary\n",
    "data_ndarrays = {}\n",
    "for key, value in data.items():\n",
    "    # Detect if the key is 'Info' and skip it\n",
    "    if key != 'Info':\n",
    "        # Drop the 'ID' and 'Task' columns if they exist\n",
    "        if 'ID' in data[key].columns:\n",
    "            data[key].drop('ID', axis=1, inplace=True)\n",
    "        if 'Task' in data[key].columns:\n",
    "            data[key].drop('Task', axis=1, inplace=True)  \n",
    "        data_ndarrays[key] = value.values\n",
    "    else:\n",
    "        data_ndarrays[key] = value\n",
    "\n",
    "label_ndarrays = {}\n",
    "label_ndarrays_CInonCI = {}\n",
    "for key, value in info.items():\n",
    "    if key != 'Info':\n",
    "        label_ndarrays[key] = value['Label'].values\n",
    "        label_ndarrays_CInonCI[key] = value['Label'].values\n",
    "    else:\n",
    "        label_ndarrays[key] = value\n",
    "        label_ndarrays_CInonCI[key] = value\n",
    "\n",
    "# Replace 2 with 1 in label_ndarrays\n",
    "label_ndarrays_CInonCI = {key: np.where(value == 2, 1, value) for key, value in label_ndarrays_CInonCI.items()}\n",
    "\n",
    "print(label_ndarrays['train'])\n",
    "label_ndarrays_CInonCI['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying the data using LDA and LOO-CV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "from feature_related import feature_selection as f_selection  # Import the feature_selection module\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('lda', lda)\n",
    "])\n",
    "\n",
    "looCV = LeaveOneOut()\n",
    "\n",
    "# Train & Valid data organizing\n",
    "data_train_valid = np.concatenate([data_ndarrays['train'], data_ndarrays['valid']], axis=0)\n",
    "label_train_valid = np.concatenate([label_ndarrays_CInonCI['train'], label_ndarrays_CInonCI['valid']], axis=0)\n",
    "\n",
    "# Feature selection - filter_based\n",
    "f_selection = f_selection()\n",
    "fisher_scores = f_selection.fisher_score(data_train_valid, label_train_valid)\n",
    "fisher_idx = np.argsort(fisher_scores)[::-1] # sort in descending order\n",
    "\n",
    "# Loop through the indices the looCV.split() method returns\n",
    "df_accCV =list()\n",
    "max_dim = int(label_train_valid.shape[0]-1)  # Define the maximum number of features\n",
    "for idx_AOFI in tqdm(range(1, max_dim + 1)):\n",
    "    data_CV = data_train_valid[:, fisher_idx[:idx_AOFI]]\n",
    "    # print(data_CV.shape)\n",
    "    accCV = 0\n",
    "    predicted_labels = []\n",
    "    for i, (train_idx, valid_idx) in enumerate(looCV.split(data_train_valid)):\n",
    "        # Train the model\n",
    "        pipeline.fit(data_CV[train_idx], label_train_valid[train_idx])\n",
    "        # Test the model\n",
    "        predicted_labels.append(pipeline.predict(data_CV[valid_idx]))\n",
    "        # print('Prediction:', score, end=' ')\n",
    "        # print('True label:', label_ndarrays_CInonCI['train'][valid_idx])\n",
    "\n",
    "    predicted_labels = np.concatenate(predicted_labels)\n",
    "\n",
    "    # Calculate true positive rate (TPR)\n",
    "    TP = sum((label_train_valid == 1) & (predicted_labels == 1))\n",
    "    FN = sum((label_train_valid == 1) & (predicted_labels == 0))\n",
    "    TPR = TP / (TP + FN)\n",
    "\n",
    "    # Calculate false positive rate (FPR)\n",
    "    FP = sum((label_train_valid == 0) & (predicted_labels == 1))\n",
    "    TN = sum((label_train_valid == 0) & (predicted_labels == 0))\n",
    "    TNR = TN / (FP + TN)\n",
    "\n",
    "    balanced_accuracy = (TPR + TNR) / 2\n",
    "    # Append the loop index and accCV to the dataframe\n",
    "    df_accCV.append({'#features': idx_AOFI, 'accCV(balanced)': balanced_accuracy})\n",
    "\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = '../Results/Classification/LDA/AOFI/'\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "if not os.path.exists(folder_path):    os.makedirs(folder_path)\n",
    "\n",
    "# Save the dataframe to csv\n",
    "df_accCV = pd.DataFrame(df_accCV) # Convert the list to a dataframe\n",
    "df_accCV.to_csv(os.path.join(folder_path, 'df_accCV.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [07:59<00:00,  4.57s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# Create the SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "gamma_range = np.linspace(-100,100,41) #-100,-95,...,95,100\n",
    "gamma_range = 1.05**gamma_range # 1.05^-100,1.05^-95,...,1.05^95,1.05^100\n",
    "gamma_range = 1 / 2*(np.square(gamma_range))  # gamma = 1 / (2*sigma)^2, based on the SVC documentation\n",
    "gamma_range = gamma_range.tolist()\n",
    "C_range = [1, 10, 100, 500, 1000]\n",
    "param_grid = {'C': C_range, 'gamma': gamma_range, 'kernel': ['rbf']}\n",
    "\n",
    "# Loop through the indices the looCV.split() method returns\n",
    "df_accCV =list()\n",
    "max_dim = int(label_train_valid.shape[0]/2)  # Define the maximum number of features\n",
    "for idx_AOFI in tqdm(range(1, max_dim + 1)):\n",
    "    data_CV = data_train_valid[:, fisher_idx[:idx_AOFI]]\n",
    "    \n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=10)\n",
    "\n",
    "    # Train the model with gridsearchCV\n",
    "    grid_search.fit(data_CV, label_train_valid)\n",
    "\n",
    "    # Get the best parameters and best score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    df_accCV.append({'#features': idx_AOFI, 'best_score': best_score, 'best_params': best_params})\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = '../Results/Classification/SVM/AOFI_smallerGrid_10Fold/'\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "if not os.path.exists(folder_path):    os.makedirs(folder_path)\n",
    "\n",
    "# Save the dataframe to csv\n",
    "df_accCV = pd.DataFrame(df_accCV) # Convert the list to a dataframe\n",
    "df_accCV.to_csv(os.path.join(folder_path, 'df_accCV.csv'), index=False)\n",
    "\n",
    "# Create a dictionary with 'fisher_idx' and 'feature_type' as keys\n",
    "fisher_idx_DF = {'fisher_idx': fisher_idx, 'feature_type': data['train'].columns[fisher_idx]}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "fisher_idx_DF = pd.DataFrame(fisher_idx_DF)\n",
    "\n",
    "# Save the DataFrame to a .csv file\n",
    "fisher_idx_DF.to_csv(os.path.join(folder_path, 'fisher_idx_series.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFGPU_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
