{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Task</th>\n",
       "      <th>Coh_delta_T5-T3</th>\n",
       "      <th>Coh_delta_T5-F7</th>\n",
       "      <th>Coh_delta_T5-O1</th>\n",
       "      <th>Coh_delta_T5-Cp3</th>\n",
       "      <th>Coh_delta_T5-Fc3</th>\n",
       "      <th>Coh_delta_T5-Fp1</th>\n",
       "      <th>Coh_delta_T5-Fcz</th>\n",
       "      <th>Coh_delta_T5-Cpz</th>\n",
       "      <th>...</th>\n",
       "      <th>Corr_Cp4-O2</th>\n",
       "      <th>Corr_Cp4-F8</th>\n",
       "      <th>Corr_Cp4-T4</th>\n",
       "      <th>Corr_Cp4-T6</th>\n",
       "      <th>Corr_O2-F8</th>\n",
       "      <th>Corr_O2-T4</th>\n",
       "      <th>Corr_O2-T6</th>\n",
       "      <th>Corr_F8-T4</th>\n",
       "      <th>Corr_F8-T6</th>\n",
       "      <th>Corr_T4-T6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NTUH_0004</td>\n",
       "      <td>Rest</td>\n",
       "      <td>0.739369</td>\n",
       "      <td>0.701256</td>\n",
       "      <td>0.759699</td>\n",
       "      <td>0.741979</td>\n",
       "      <td>0.482286</td>\n",
       "      <td>0.165846</td>\n",
       "      <td>0.110406</td>\n",
       "      <td>0.291415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765194</td>\n",
       "      <td>0.567678</td>\n",
       "      <td>0.708985</td>\n",
       "      <td>0.803361</td>\n",
       "      <td>0.404576</td>\n",
       "      <td>0.661505</td>\n",
       "      <td>0.919683</td>\n",
       "      <td>0.717551</td>\n",
       "      <td>0.520095</td>\n",
       "      <td>0.783182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CGMHKL_0002</td>\n",
       "      <td>Rest</td>\n",
       "      <td>0.814006</td>\n",
       "      <td>0.304687</td>\n",
       "      <td>0.923202</td>\n",
       "      <td>0.659374</td>\n",
       "      <td>0.235388</td>\n",
       "      <td>0.263912</td>\n",
       "      <td>0.233530</td>\n",
       "      <td>0.494247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594284</td>\n",
       "      <td>0.311152</td>\n",
       "      <td>0.518404</td>\n",
       "      <td>0.688676</td>\n",
       "      <td>0.192620</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.762633</td>\n",
       "      <td>0.449235</td>\n",
       "      <td>0.383965</td>\n",
       "      <td>0.661592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NTUH_0011</td>\n",
       "      <td>Rest</td>\n",
       "      <td>0.756742</td>\n",
       "      <td>0.273296</td>\n",
       "      <td>0.838798</td>\n",
       "      <td>0.849737</td>\n",
       "      <td>0.255621</td>\n",
       "      <td>0.145847</td>\n",
       "      <td>0.172226</td>\n",
       "      <td>0.420560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648830</td>\n",
       "      <td>0.432738</td>\n",
       "      <td>0.592082</td>\n",
       "      <td>0.750562</td>\n",
       "      <td>0.244982</td>\n",
       "      <td>0.558273</td>\n",
       "      <td>0.891249</td>\n",
       "      <td>0.643338</td>\n",
       "      <td>0.400314</td>\n",
       "      <td>0.701446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CGMHKL_0005</td>\n",
       "      <td>Rest</td>\n",
       "      <td>0.755170</td>\n",
       "      <td>0.332022</td>\n",
       "      <td>0.721507</td>\n",
       "      <td>0.514757</td>\n",
       "      <td>0.138277</td>\n",
       "      <td>0.259687</td>\n",
       "      <td>0.193953</td>\n",
       "      <td>0.307590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614496</td>\n",
       "      <td>0.296134</td>\n",
       "      <td>0.239646</td>\n",
       "      <td>0.736570</td>\n",
       "      <td>0.284022</td>\n",
       "      <td>0.345613</td>\n",
       "      <td>0.818268</td>\n",
       "      <td>0.302792</td>\n",
       "      <td>0.371725</td>\n",
       "      <td>0.428486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CGMHKL_0004</td>\n",
       "      <td>Rest</td>\n",
       "      <td>0.651956</td>\n",
       "      <td>0.284282</td>\n",
       "      <td>0.611058</td>\n",
       "      <td>0.573420</td>\n",
       "      <td>0.240884</td>\n",
       "      <td>0.169931</td>\n",
       "      <td>0.155174</td>\n",
       "      <td>0.229636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347948</td>\n",
       "      <td>0.311138</td>\n",
       "      <td>0.429988</td>\n",
       "      <td>0.502850</td>\n",
       "      <td>0.242461</td>\n",
       "      <td>0.377395</td>\n",
       "      <td>0.727081</td>\n",
       "      <td>0.626247</td>\n",
       "      <td>0.420851</td>\n",
       "      <td>0.575280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  Task  Coh_delta_T5-T3  Coh_delta_T5-F7  Coh_delta_T5-O1  \\\n",
       "0    NTUH_0004  Rest         0.739369         0.701256         0.759699   \n",
       "1  CGMHKL_0002  Rest         0.814006         0.304687         0.923202   \n",
       "2    NTUH_0011  Rest         0.756742         0.273296         0.838798   \n",
       "3  CGMHKL_0005  Rest         0.755170         0.332022         0.721507   \n",
       "4  CGMHKL_0004  Rest         0.651956         0.284282         0.611058   \n",
       "\n",
       "   Coh_delta_T5-Cp3  Coh_delta_T5-Fc3  Coh_delta_T5-Fp1  Coh_delta_T5-Fcz  \\\n",
       "0          0.741979          0.482286          0.165846          0.110406   \n",
       "1          0.659374          0.235388          0.263912          0.233530   \n",
       "2          0.849737          0.255621          0.145847          0.172226   \n",
       "3          0.514757          0.138277          0.259687          0.193953   \n",
       "4          0.573420          0.240884          0.169931          0.155174   \n",
       "\n",
       "   Coh_delta_T5-Cpz  ...  Corr_Cp4-O2  Corr_Cp4-F8  Corr_Cp4-T4  Corr_Cp4-T6  \\\n",
       "0          0.291415  ...     0.765194     0.567678     0.708985     0.803361   \n",
       "1          0.494247  ...     0.594284     0.311152     0.518404     0.688676   \n",
       "2          0.420560  ...     0.648830     0.432738     0.592082     0.750562   \n",
       "3          0.307590  ...     0.614496     0.296134     0.239646     0.736570   \n",
       "4          0.229636  ...     0.347948     0.311138     0.429988     0.502850   \n",
       "\n",
       "   Corr_O2-F8  Corr_O2-T4  Corr_O2-T6  Corr_F8-T4  Corr_F8-T6  Corr_T4-T6  \n",
       "0    0.404576    0.661505    0.919683    0.717551    0.520095    0.783182  \n",
       "1    0.192620    0.477090    0.762633    0.449235    0.383965    0.661592  \n",
       "2    0.244982    0.558273    0.891249    0.643338    0.400314    0.701446  \n",
       "3    0.284022    0.345613    0.818268    0.302792    0.371725    0.428486  \n",
       "4    0.242461    0.377395    0.727081    0.626247    0.420851    0.575280  \n",
       "\n",
       "[5 rows x 13052 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Specify the path to the pickle file\n",
    "data_file_path = '../Dataset/Dementia_paper_dataset_data.pkl'\n",
    "info_file_path = '../Dataset/Dementia_paper_dataset_info.pkl'\n",
    "\n",
    "# Open the pickle file in read mode\n",
    "with open(data_file_path, 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    data = pd.read_pickle(file)\n",
    "\n",
    "# Open the pickle file in read mode\n",
    "with open(info_file_path, 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    info = pd.read_pickle(file)\n",
    "\n",
    "# Display the first few rows of the data\n",
    "data['train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the file path for the Excel file\n",
    "excel_file_path = '../Dataset/Dementia_paper_dataset_data.xlsx'\n",
    "\n",
    "# Create an ExcelWriter object\n",
    "writer = pd.ExcelWriter(excel_file_path, engine='openpyxl')\n",
    "\n",
    "# Loop through the dictionary items and save each dataframe as a separate sheet in the Excel file\n",
    "for key, value in info.items():\n",
    "    if key != 'Info':\n",
    "        # Write the dataframe to the Excel file\n",
    "        value.to_excel(writer, sheet_name=key, index=False)\n",
    "\n",
    "# Save the Excel file\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HC: 0, MCI: 1, Dementia: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 2 1 1 2 1 1 1\n",
      " 2 2 2 0 1 2 2 1 1 0 1 0 0 0 0 2 0 0 1 0 1 0 0 0 0 1 2 1 0 1 0 0 2 2 1 0 2\n",
      " 2 0 2 2 0 1 1 0 0 0 0 2 0 2 0 1 2 0 2 1 1 1 0 1 2 0 1 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 1 1 2 2 0 0 0 1 0 2 0 0 2 0 1 0 0 1 0 2 0 1 1 1 0 0 0 0 2\n",
      " 2 0 0 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Dataframe to numpy array in each dictionary\n",
    "data_ndarrays = {}\n",
    "for key, value in data.items():\n",
    "    # Detect if the key is 'Info' and skip it\n",
    "    if key != 'Info':\n",
    "        # Drop the 'ID' and 'Task' columns if they exist\n",
    "        if 'ID' in data[key].columns:\n",
    "            data[key].drop('ID', axis=1, inplace=True)\n",
    "        if 'Task' in data[key].columns:\n",
    "            data[key].drop('Task', axis=1, inplace=True)  \n",
    "        data_ndarrays[key] = value.values\n",
    "    else:\n",
    "        data_ndarrays[key] = value\n",
    "\n",
    "label_ndarrays = {}\n",
    "label_ndarrays_CInonCI = {}\n",
    "for key, value in info.items():\n",
    "    if key != 'Info':\n",
    "        label_ndarrays[key] = value['Label'].values\n",
    "        label_ndarrays_CInonCI[key] = value['Label'].values\n",
    "    else:\n",
    "        label_ndarrays[key] = value\n",
    "        label_ndarrays_CInonCI[key] = value\n",
    "\n",
    "# Replace 2 with 1 in label_ndarrays\n",
    "label_ndarrays_CInonCI = {key: np.where(value == 2, 1, value) for key, value in label_ndarrays_CInonCI.items()}\n",
    "\n",
    "print(label_ndarrays['train'])\n",
    "label_ndarrays_CInonCI['train']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Count the number of SCD-MCI or SCD-Control in the data set\n",
    "group_ndarrays = {}\n",
    "n_SCD_MCI = {}\n",
    "n_SCD_Control = {}\n",
    "for key, value in info.items():\n",
    "    if key != 'Info':\n",
    "        group_ndarrays[key] = value['Group'].values\n",
    "        n_SCD_MCI[key] = np.sum(group_ndarrays[key] == 'SCD-MCI') \n",
    "        n_SCD_Control[key] = np.sum(group_ndarrays[key] == 'SCD-Control') \n",
    "    else:\n",
    "        group_ndarrays[key] = value\n",
    "\n",
    "print(n_SCD_MCI['train'])\n",
    "print(n_SCD_Control['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210/210 [08:57<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# data organizing and feature selection(fisher score)\n",
    "import os\n",
    "from feature_related import feature_selection as f_selection  # Import the feature_selection module\n",
    "\n",
    "# Train & Valid data organizing\n",
    "data_train_valid = np.concatenate([data_ndarrays['train'], data_ndarrays['valid']], axis=0)\n",
    "label_train_valid = np.concatenate([label_ndarrays_CInonCI['train'], label_ndarrays_CInonCI['valid']], axis=0)\n",
    "\n",
    "# Independent test data organizing\n",
    "data_test = data_ndarrays['test']\n",
    "label_test = label_ndarrays_CInonCI['test']\n",
    "\n",
    "# Feature selection - filter_based\n",
    "f_selection = f_selection()\n",
    "fisher_scores = f_selection.fisher_score(data_train_valid, label_train_valid)\n",
    "fisher_idx = np.argsort(fisher_scores)[::-1] # sort in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [00:05<00:00, 19.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# classifying the data using LDA and LOO-CV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "from feature_related import feature_selection as f_selection  # Import the feature_selection module\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# Create the SVM classifier\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {'solver': ['svd']}\n",
    "\n",
    "\n",
    "\n",
    "df_accCV =list()\n",
    "max_dim = int(label_train_valid.shape[0]/2)  # Define the maximum number of features\n",
    "for idx_AOFI in tqdm(range(1, max_dim+1)):\n",
    "    data_CV = data_train_valid[:, fisher_idx[:idx_AOFI]]\n",
    "    \n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=lda, param_grid=param_grid, cv=10)\n",
    "\n",
    "    # Train the model with gridsearchCV\n",
    "    grid_search.fit(data_CV, label_train_valid)\n",
    "\n",
    "    # Get the best parameters and best score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_score = best_model.score(data_test[:, fisher_idx[:idx_AOFI]], label_test)\n",
    "    \n",
    "    # Append the loop index and accCV to the dataframe\n",
    "    df_accCV.append({'#features': idx_AOFI, 'best_CVscore': best_score, 'best_params': best_params, 'test_score': test_score})\n",
    "    \n",
    "\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = '../Results/Classification/LDA/10Fold_test/'\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "if not os.path.exists(folder_path):    os.makedirs(folder_path)\n",
    "\n",
    "# Save the dataframe to csv\n",
    "df_accCV = pd.DataFrame(df_accCV) # Convert the list to a dataframe\n",
    "df_accCV.to_csv(os.path.join(folder_path, 'df_accCV.csv'), index=False)\n",
    "\n",
    "# Create a dictionary with 'fisher_idx' and 'feature_type' as keys\n",
    "fisher_idx_DF = {'fisher_idx': fisher_idx, 'feature_type': data['train'].columns[fisher_idx]}\n",
    "fisher_idx_DF = pd.DataFrame(fisher_idx_DF) # Convert the dictionary to a DataFrame\n",
    "fisher_idx_DF.to_csv(os.path.join(folder_path, 'fisher_idx_series.csv'), index=False) # Save the DataFrame to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [08:30<00:00,  4.86s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "# Create the SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "gamma_range = np.linspace(-100,100,41) #-100,-95,...,95,100\n",
    "gamma_range = 1.05**gamma_range # 1.05^-100,1.05^-95,...,1.05^95,1.05^100\n",
    "gamma_range = 1 / 2*(np.square(gamma_range))  # gamma = 1 / (2*sigma)^2, based on the SVC documentation\n",
    "gamma_range = gamma_range.tolist()\n",
    "C_range = [1, 10, 100, 500, 1000]\n",
    "param_grid = {'C': C_range, 'gamma': gamma_range, 'kernel': ['rbf']}\n",
    "\n",
    "\n",
    "df_accCV =list()\n",
    "max_dim = int(label_train_valid.shape[0]/2)  # Define the maximum number of features\n",
    "for idx_AOFI in tqdm(range(1, max_dim + 1)):\n",
    "    data_CV = data_train_valid[:, fisher_idx[:idx_AOFI]]\n",
    "    \n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=10)\n",
    "\n",
    "    # Train the model with gridsearchCV\n",
    "    grid_search.fit(data_CV, label_train_valid)\n",
    "\n",
    "    # Get the best parameters and best score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_score = best_model.score(data_test[:, fisher_idx[:idx_AOFI]], label_test)\n",
    "    \n",
    "    # Append the loop index and accCV to the dataframe\n",
    "    df_accCV.append({'#features': idx_AOFI, 'best_CVscore': best_score, 'best_params': best_params, 'test_score': test_score})\n",
    "    \n",
    "\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = '../Results/Classification/SVM/AOFI_smallerGrid_10Fold/'\n",
    "\n",
    "# Create the folder if it does not exist\n",
    "if not os.path.exists(folder_path):    os.makedirs(folder_path)\n",
    "\n",
    "# Save the dataframe to csv\n",
    "df_accCV = pd.DataFrame(df_accCV) # Convert the list to a dataframe\n",
    "df_accCV.to_csv(os.path.join(folder_path, 'df_accCV.csv'), index=False)\n",
    "\n",
    "# Create a dictionary with 'fisher_idx' and 'feature_type' as keys\n",
    "fisher_idx_DF = {'fisher_idx': fisher_idx, 'feature_type': data['train'].columns[fisher_idx]}\n",
    "fisher_idx_DF = pd.DataFrame(fisher_idx_DF) # Convert the dictionary to a DataFrame\n",
    "fisher_idx_DF.to_csv(os.path.join(folder_path, 'fisher_idx_series.csv'), index=False) # Save the DataFrame to a .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFGPU_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
