{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Task</th>\n",
       "      <th>Coh_delta_T5-T3</th>\n",
       "      <th>Coh_delta_T5-F7</th>\n",
       "      <th>Coh_delta_T5-O1</th>\n",
       "      <th>Coh_delta_T5-Cp3</th>\n",
       "      <th>Coh_delta_T5-Fc3</th>\n",
       "      <th>Coh_delta_T5-Fp1</th>\n",
       "      <th>Coh_delta_T5-Fcz</th>\n",
       "      <th>Coh_delta_T5-Cpz</th>\n",
       "      <th>...</th>\n",
       "      <th>Corr_Cp4-O2</th>\n",
       "      <th>Corr_Cp4-F8</th>\n",
       "      <th>Corr_Cp4-T4</th>\n",
       "      <th>Corr_Cp4-T6</th>\n",
       "      <th>Corr_O2-F8</th>\n",
       "      <th>Corr_O2-T4</th>\n",
       "      <th>Corr_O2-T6</th>\n",
       "      <th>Corr_F8-T4</th>\n",
       "      <th>Corr_F8-T6</th>\n",
       "      <th>Corr_T4-T6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NTUH_0004</td>\n",
       "      <td>Rest</td>\n",
       "      <td>0.739369</td>\n",
       "      <td>0.701256</td>\n",
       "      <td>0.759699</td>\n",
       "      <td>0.741979</td>\n",
       "      <td>0.482286</td>\n",
       "      <td>0.165846</td>\n",
       "      <td>0.110406</td>\n",
       "      <td>0.291415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765194</td>\n",
       "      <td>0.567678</td>\n",
       "      <td>0.708985</td>\n",
       "      <td>0.803361</td>\n",
       "      <td>0.404576</td>\n",
       "      <td>0.661505</td>\n",
       "      <td>0.919683</td>\n",
       "      <td>0.717551</td>\n",
       "      <td>0.520095</td>\n",
       "      <td>0.783182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CGMHKL_0002</td>\n",
       "      <td>Rest</td>\n",
       "      <td>0.814006</td>\n",
       "      <td>0.304687</td>\n",
       "      <td>0.923202</td>\n",
       "      <td>0.659374</td>\n",
       "      <td>0.235388</td>\n",
       "      <td>0.263912</td>\n",
       "      <td>0.233530</td>\n",
       "      <td>0.494247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594284</td>\n",
       "      <td>0.311152</td>\n",
       "      <td>0.518404</td>\n",
       "      <td>0.688676</td>\n",
       "      <td>0.192620</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>0.762633</td>\n",
       "      <td>0.449235</td>\n",
       "      <td>0.383965</td>\n",
       "      <td>0.661592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NTUH_0011</td>\n",
       "      <td>Rest</td>\n",
       "      <td>0.756742</td>\n",
       "      <td>0.273296</td>\n",
       "      <td>0.838798</td>\n",
       "      <td>0.849737</td>\n",
       "      <td>0.255621</td>\n",
       "      <td>0.145847</td>\n",
       "      <td>0.172226</td>\n",
       "      <td>0.420560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648830</td>\n",
       "      <td>0.432738</td>\n",
       "      <td>0.592082</td>\n",
       "      <td>0.750562</td>\n",
       "      <td>0.244982</td>\n",
       "      <td>0.558273</td>\n",
       "      <td>0.891249</td>\n",
       "      <td>0.643338</td>\n",
       "      <td>0.400314</td>\n",
       "      <td>0.701446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CGMHKL_0005</td>\n",
       "      <td>Rest</td>\n",
       "      <td>0.755170</td>\n",
       "      <td>0.332022</td>\n",
       "      <td>0.721507</td>\n",
       "      <td>0.514757</td>\n",
       "      <td>0.138277</td>\n",
       "      <td>0.259687</td>\n",
       "      <td>0.193953</td>\n",
       "      <td>0.307590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614496</td>\n",
       "      <td>0.296134</td>\n",
       "      <td>0.239646</td>\n",
       "      <td>0.736570</td>\n",
       "      <td>0.284022</td>\n",
       "      <td>0.345613</td>\n",
       "      <td>0.818268</td>\n",
       "      <td>0.302792</td>\n",
       "      <td>0.371725</td>\n",
       "      <td>0.428486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CGMHKL_0004</td>\n",
       "      <td>Rest</td>\n",
       "      <td>0.651956</td>\n",
       "      <td>0.284282</td>\n",
       "      <td>0.611058</td>\n",
       "      <td>0.573420</td>\n",
       "      <td>0.240884</td>\n",
       "      <td>0.169931</td>\n",
       "      <td>0.155174</td>\n",
       "      <td>0.229636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347948</td>\n",
       "      <td>0.311138</td>\n",
       "      <td>0.429988</td>\n",
       "      <td>0.502850</td>\n",
       "      <td>0.242461</td>\n",
       "      <td>0.377395</td>\n",
       "      <td>0.727081</td>\n",
       "      <td>0.626247</td>\n",
       "      <td>0.420851</td>\n",
       "      <td>0.575280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  Task  Coh_delta_T5-T3  Coh_delta_T5-F7  Coh_delta_T5-O1  \\\n",
       "0    NTUH_0004  Rest         0.739369         0.701256         0.759699   \n",
       "1  CGMHKL_0002  Rest         0.814006         0.304687         0.923202   \n",
       "2    NTUH_0011  Rest         0.756742         0.273296         0.838798   \n",
       "3  CGMHKL_0005  Rest         0.755170         0.332022         0.721507   \n",
       "4  CGMHKL_0004  Rest         0.651956         0.284282         0.611058   \n",
       "\n",
       "   Coh_delta_T5-Cp3  Coh_delta_T5-Fc3  Coh_delta_T5-Fp1  Coh_delta_T5-Fcz  \\\n",
       "0          0.741979          0.482286          0.165846          0.110406   \n",
       "1          0.659374          0.235388          0.263912          0.233530   \n",
       "2          0.849737          0.255621          0.145847          0.172226   \n",
       "3          0.514757          0.138277          0.259687          0.193953   \n",
       "4          0.573420          0.240884          0.169931          0.155174   \n",
       "\n",
       "   Coh_delta_T5-Cpz  ...  Corr_Cp4-O2  Corr_Cp4-F8  Corr_Cp4-T4  Corr_Cp4-T6  \\\n",
       "0          0.291415  ...     0.765194     0.567678     0.708985     0.803361   \n",
       "1          0.494247  ...     0.594284     0.311152     0.518404     0.688676   \n",
       "2          0.420560  ...     0.648830     0.432738     0.592082     0.750562   \n",
       "3          0.307590  ...     0.614496     0.296134     0.239646     0.736570   \n",
       "4          0.229636  ...     0.347948     0.311138     0.429988     0.502850   \n",
       "\n",
       "   Corr_O2-F8  Corr_O2-T4  Corr_O2-T6  Corr_F8-T4  Corr_F8-T6  Corr_T4-T6  \n",
       "0    0.404576    0.661505    0.919683    0.717551    0.520095    0.783182  \n",
       "1    0.192620    0.477090    0.762633    0.449235    0.383965    0.661592  \n",
       "2    0.244982    0.558273    0.891249    0.643338    0.400314    0.701446  \n",
       "3    0.284022    0.345613    0.818268    0.302792    0.371725    0.428486  \n",
       "4    0.242461    0.377395    0.727081    0.626247    0.420851    0.575280  \n",
       "\n",
       "[5 rows x 13052 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Specify the path to the pickle file\n",
    "data_file_path = '../Dataset/Dementia_paper_dataset_data.pkl'\n",
    "info_file_path = '../Dataset/Dementia_paper_dataset_info.pkl'\n",
    "\n",
    "# Open the pickle file in read mode\n",
    "with open(data_file_path, 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    data = pd.read_pickle(file)\n",
    "\n",
    "# Open the pickle file in read mode\n",
    "with open(info_file_path, 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    info = pd.read_pickle(file)\n",
    "\n",
    "# Display the first few rows of the data\n",
    "data['train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   >> ['Coh' 'Corr' 'DFA' 'HFD100' 'ID' 'KFD' 'LogBP' 'PLI' 'PLV' \n",
    "#       'PLZC' 'RP-I' 'RP-II' 'RP-III' 'RatioPower' 'SampEn-I' 'SampEn-II' 'Task']\n",
    "\n",
    "RP3 = data['train']['RP-III_beta_T5-O1']\n",
    "LogBP_T5 = data['train']['LogBP_beta_T5']\n",
    "LogBP_O1 = data['train']['LogBP_beta_O1']\n",
    "RP3_alt = LogBP_T5-LogBP_O1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -2.105120e-11\n",
       "1      2.105285e-11\n",
       "2     -5.262388e-12\n",
       "3      5.262679e-12\n",
       "4      1.052551e-11\n",
       "           ...     \n",
       "148   -1.052844e-11\n",
       "149    1.052600e-11\n",
       "150   -1.578858e-11\n",
       "151   -1.052769e-11\n",
       "152    4.996004e-16\n",
       "Length: 153, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "RP3- RP3_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. 10. 20. 30. 40. 50. 60. 70. 80. 90.]\n",
      "[ 0. 18. 36. 54. 72.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.linspace(0, 90, 10) # create a linearly spaced array\n",
    "\n",
    "# Calculate the quantiles of all features\n",
    "q = 5 # number of quantiles\n",
    "quantiles = np.array([i/q for i in range(q)]) # create quantiles\n",
    "quantilized_features = np.quantile(X, quantiles, axis=0) # calculate quantiles of each feature\n",
    "\n",
    "print(X)\n",
    "print(quantilized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n",
      "18 | 16 | 0.9\n",
      "72 | 6 | 0.97\n",
      "(1000, 98)\n",
      "{72, 18}\n"
     ]
    }
   ],
   "source": [
    "from feature_related import feature_selection as f_selection\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=100, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "\n",
    "fs = f_selection()\n",
    "\n",
    "print(X.shape)\n",
    "X, drop = fs.remove_collinear_features(X, 0.8)\n",
    "print(X.shape)\n",
    "print(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "a[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.randn(20, 10)\n",
    "\n",
    "y = np.zeros(20)\n",
    "y[:10] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_related import feature_selection as f_selection\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Feature selection - filter_based\n",
    "f_selection = f_selection()\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "\n",
    "# Separate the features (X) and target variable (y)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "fisher_scores = f_selection.fisher_score(X[:100,:], y[:100])\n",
    "fisher_idx = np.argsort(fisher_scores)[::-1] # sort in descending order\n",
    "\n",
    "print(\"Fisher scores: \", fisher_scores)\n",
    "print(\"Fisher indexes: \", fisher_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.linspace(0, 23, 24).reshape(3, 1,2, 4)\n",
    "print(a)\n",
    "for aa in a:\n",
    "    print(aa)\n",
    "    print(np.average(aa, axis=1))\n",
    "    print()\n",
    "\n",
    "print(np.transpose(a, [1,2,0,3]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_EEGNet(Y, evaluation, folder_path, csv_name:str):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    folder_path = folder_path\n",
    "    # Create the folder if it does not exist\n",
    "    if not os.path.exists(folder_path):    os.makedirs(folder_path)\n",
    "        \n",
    "    evaluation_DF = pd.DataFrame(evaluation, index=[0])\n",
    "    evaluation_DF.to_csv(os.path.join(folder_path, 'evaluation', csv_name, '.csv'), index=False)\n",
    "\n",
    "    Y_DF = pd.DataFrame(Y)\n",
    "    Y_DF.to_csv(os.path.join(folder_path, 'Y', csv_name, '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary\n",
    "my_dict = {}\n",
    "\n",
    "# Define keys for the dictionary\n",
    "keys = ['key1', 'key2', 'key3']\n",
    "\n",
    "# Initialize lists for each key\n",
    "for key in keys:\n",
    "    my_dict[key] = []\n",
    "\n",
    "# Example elements to append\n",
    "elements = {\n",
    "    'key1': [1, 2, 3],\n",
    "    'key2': ['a', 'b', 'c'],\n",
    "    'key3': ['x', 'y', 'z']\n",
    "}\n",
    "\n",
    "# Append elements to the corresponding lists using a for loop\n",
    "for key, values in elements.items():\n",
    "    my_dict[key].extend(values)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(my_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "a = {'hello': 'world'}\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = {'dropout_rate': [0.5, 0.75],\n",
    "               'lr': [0.001, 0.01]}\n",
    "grid_search['dropout_rate']\n",
    "for dropout_rate in grid_search['dropout_rate']:\n",
    "    print(dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "signal = np.arange(0, 3000).reshape(30,100)\n",
    "print(signal)\n",
    "\n",
    "CZ_seq = 2\n",
    "a = signal-signal[CZ_seq,:]\n",
    "print(a)\n",
    "a = np.delete(a, CZ_seq, axis=0)\n",
    "print(a.shape)\n",
    "\n",
    "a_scale = (a-np.min(a))/(np.max(a)-np.min(a))\n",
    "print(a_scale)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(a_scale, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "fftfreq = np.fft.fftfreq(1500, d=1/500)\n",
    "mask = (fftfreq >= 2) & (fftfreq < 4)\n",
    "print(mask)\n",
    "\n",
    "a = np.arange(0, 1500)\n",
    "a = a[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = np.arange(0, 12).reshape(3,4)\n",
    "print(a)\n",
    "dfa = pd.DataFrame(a)\n",
    "print(dfa)\n",
    "\n",
    "b = np.arange(12, 24).reshape(3,4)\n",
    "print(b)\n",
    "dfb = pd.DataFrame(b)\n",
    "print(dfb)\n",
    "\n",
    "ddd = {'a':dfa, 'b':dfb}\n",
    "print(ddd)\n",
    "\n",
    "meanddd = {}\n",
    "\n",
    "for key, value in ddd.items(): \n",
    "    meanddd[key] = value.mean()\n",
    "print(meanddd)\n",
    "meanddd = pd.DataFrame(meanddd).transpose()\n",
    "print(meanddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale, MinMaxScaler\n",
    "a = np.arange(0, 12).reshape(3,4)\n",
    "print(a)\n",
    "\n",
    "\n",
    "aa0 = minmax_scale(a, axis=0)\n",
    "print(aa0)\n",
    "\n",
    "aa1 = minmax_scale(a, axis=1)\n",
    "print(aa1)\n",
    "\n",
    "aa_all = minmax_scale(a.flatten()).reshape(a.shape)\n",
    "print(aa_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "\n",
    "# Separate the features (X) and target variable (y)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "clf = LDA()\n",
    "rfecv = SequentialFeatureSelector(estimator=clf, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0), n_features_to_select=2, direction='forward')\n",
    "\n",
    "param_grid = { \n",
    "    'estimator__solver': ['svd', 'lsqr']\n",
    "}\n",
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfecv, param_grid=param_grid, cv= k_fold)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "CV_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Create a SequentialFeatureSelector\n",
    "sfs = SequentialFeatureSelector(knn, k_features=(1, 4), forward=False, floating=False, scoring='accuracy', cv=5)\n",
    "\n",
    "# Fit the SequentialFeatureSelector to the training data\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Print the selected feature indices\n",
    "print(\"Selected feature indices:\", sfs.k_feature_idx_)\n",
    "\n",
    "# Print the selected feature names\n",
    "selected_features = [iris.feature_names[i] for i in sfs.k_feature_idx_]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# # Evaluate the performance of the selected features on the test set\n",
    "# accuracy = sfs.fit(X_test, y_test)\n",
    "# print(\"Accuracy:\", sfs.k_score_)\n",
    "\n",
    "print(X_train.shape)\n",
    "a = sfs.transform(X_train)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sfscv_results_df = pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "sfscv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in sfscv_results_df.iterrows():\n",
    "    print(row['feature_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the row with the highest avg_score\n",
    "highest_avg_score_index = sfscv_results_df['avg_score'].values.argsort()\n",
    "\n",
    "print(sfscv_results_df['avg_score'].values)\n",
    "highest_avg_score_index = highest_avg_score_index[-1]\n",
    "print(highest_avg_score_index)\n",
    "sfscv_results_df.iloc[highest_avg_score_index].feature_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "gamma_range = np.linspace(-100,100,41) #-100,-95,...,95,100\n",
    "gamma_range = 1.05**gamma_range # 1.05^-100,1.05^-95,...,1.05^95,1.05^100\n",
    "gamma_range = 1 / 2*(np.square(gamma_range))  # gamma = 1 / (2*sigma)^2, based on the SVC documentation\n",
    "gamma_range = gamma_range.tolist()\n",
    "C_range = [1, 10, 100, 500, 1000]\n",
    "\n",
    "# Create multiple parameter set based on all posible combination of the parameter grids\n",
    "param_grid = {'C': C_range, 'gamma': gamma_range, 'kernel': ['linear','rbf']}\n",
    "param_grid_list = list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale, scale\n",
    "import numpy as np\n",
    "\n",
    "arr = np.arange(0, 15).reshape(1,3,5)\n",
    "print('arr\\n',arr)\n",
    "\n",
    "ndatx = []\n",
    "for dx in arr:\n",
    "    print('dx\\n',dx)\n",
    "\n",
    "    scaled_arr = minmax_scale(dx, axis=0)\n",
    "    print('scaled_arr axis=0\\n',scaled_arr)\n",
    "\n",
    "    scaled_arr = minmax_scale(dx, axis=1)\n",
    "    print('scaled_arr axis=1\\n',scaled_arr)\n",
    "\n",
    "    scaled_arr = scale(dx, axis=0)\n",
    "    print('scaled_arr axis=0\\n',scaled_arr)\n",
    "\n",
    "    scaled_arr = scale(dx, axis=1)\n",
    "    print('scaled_arr axis=0\\n',scaled_arr)\n",
    "\n",
    "    mean, std = np.average(dx, axis=0), np.std(dx, axis=0)\n",
    "    ndatx.append((dx-mean)/std)\n",
    "    ndatx = np.array(ndatx)\n",
    "    print('ndatx\\n',ndatx)\n",
    "\n",
    "# scaled_arr = minmax_scale(arr[0,:,:], axis=0)\n",
    "# print('scaled_arr axis=0\\n',scaled_arr)\n",
    "\n",
    "# scaled_arr = minmax_scale(arr[0,:,:], axis=1)\n",
    "# print('scaled_arr axis=1\\n',scaled_arr)\n",
    "\n",
    "# scaled_arr = scale(arr[0,:,:], axis=0)\n",
    "# print('scaled_arr axis=0\\n',scaled_arr)\n",
    "\n",
    "# scaled_arr = scale(arr[0,:,:], axis=1)\n",
    "# print('scaled_arr axis=0\\n',scaled_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale, scale\n",
    "import numpy as np\n",
    "\n",
    "arr = np.arange(0, 15).reshape(1,3,5)\n",
    "arr2 = arr+15\n",
    "arr = np.concatenate((arr, arr2), axis=0).reshape(3,2,5)\n",
    "print('arr\\n',arr)\n",
    "\n",
    "for dx in arr:\n",
    "    print('dx\\n',dx)\n",
    "\n",
    "scaled_arr = minmax_scale(arr[0,:,:], axis=0)\n",
    "print('scaled_arr axis=0\\n',scaled_arr)\n",
    "\n",
    "scaled_arr = minmax_scale(arr[0,:,:], axis=1)\n",
    "print('scaled_arr axis=1\\n',scaled_arr)\n",
    "\n",
    "scaled_arr = scale(arr[0,:,:], axis=0)\n",
    "print('scaled_arr axis=0\\n',scaled_arr)\n",
    "\n",
    "scaled_arr = scale(arr[0,:,:], axis=1)\n",
    "print('scaled_arr axis=0\\n',scaled_arr)\n",
    "\n",
    "arr.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy.signal import decimate, butter, lfilter\n",
    "from scipy.stats import zscore\n",
    "\n",
    "class Data_Preprocess_EEGNet:\n",
    "    def __init__(self, data, info, **kwargs):\n",
    "        self.data = data\n",
    "        self.info = info\n",
    "        \n",
    "\n",
    "        # use **kwargs to set the new value of below args.\n",
    "        self.sec_Window = 3\n",
    "        self.sec_Overlap = 0\n",
    "        self.Fs = 500\n",
    "        self.band = [2,40] # band pass to \"band\" Hz\n",
    "        self.target_Freq = 125 # downsample to \"target_freq\" Hz\n",
    "        self.conduct_filter = True  # Whether to band-pass filter\n",
    "        self.conduct_decimate = True  # Whether to downsample\n",
    "        self.conduct_Czrrf = False # Whether to conduct CZ re-reference\n",
    "        self.Cz_sequence = 21 # sequence (position) of Cz electrode\n",
    "        self.at_each_time_point = True\n",
    "        self.across_channel = False\n",
    "        for k in kwargs.keys():\n",
    "            self.__setattr__(k, kwargs[k])\n",
    "            \n",
    "    \n",
    "    def data_preprocessing(self, data, info, sec_Window, nOverlap):\n",
    "        # Data segmentaion\n",
    "        # Note that: Data format: subject x channel x datapoint\t\n",
    "        #            Input of EEGNet: subject x 1 x channel x datapoint as an epoch\n",
    "        # Output : subject x epochs x 1 x channel x datapoint\n",
    "\n",
    "        def butter_bandpass(lowcut, highcut, fs, order=2):\n",
    "            return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "    \n",
    "        def butter_bandpass_filter(data, lowcut, highcut, fs, order=2):\n",
    "            b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "            y = lfilter(b, a, data, axis=1)\n",
    "            return y\n",
    "            \n",
    "        def epochs(signal, sec_Window, sec_Overlap):\n",
    "            q = 1\n",
    "            # Cz re-reference\n",
    "            if self.conduct_Czrrf: \n",
    "                signal = signal - signal[self.Cz_sequence, :]\n",
    "                signal = np.delete(signal, self.Cz_sequence, axis=0)\n",
    "            # band-pass filter\n",
    "            if self.conduct_filter: signal = butter_bandpass_filter(signal, self.band[0], self.band[1], self.Fs, order=5)\n",
    "            # downsample\n",
    "            if self.conduct_decimate: \n",
    "                q = int(self.Fs/self.target_Freq)\n",
    "                signal = decimate(signal, q, axis=1)\n",
    "                nWindow = sec_Window * self.target_Freq\n",
    "                nOverlap = sec_Overlap * self.target_Freq\n",
    "            else:\n",
    "                nWindow = sec_Window * self.Fs\n",
    "                nOverlap = sec_Overlap * self.Fs\n",
    "            \n",
    "            [nRow, nCol] = signal.shape\n",
    "            pt_start = 0\n",
    "            pt_end = nWindow\n",
    "            output=list()\n",
    "            while pt_end <= nCol:\n",
    "                if nWindow <= nOverlap:\n",
    "                    print('Invalid Input: nWindow == nOverlap')\n",
    "                    break\n",
    "                output.append(signal[:,pt_start:pt_end])\n",
    "                pt_start += (nWindow - nOverlap)\n",
    "                pt_end += (nWindow - nOverlap)\n",
    "            output = np.array(output)\n",
    "            # print('(epoch,channel,datapoint):',output.shape)\n",
    "            return output\n",
    "        \n",
    "        def convert_to_input(signal_3d, sec_Window, sec_Overlap):\n",
    "            [nSubject, nRow, nCol] = signal_3d.shape\n",
    "            output = list()\n",
    "            for i in range(nSubject):\n",
    "                output.append(epochs(signal_3d[i,:,:], sec_Window, sec_Overlap))\n",
    "            output = np.array(output)\n",
    "            output = output[:,:,np.newaxis,:,:]\n",
    "            print('(subject,epoch,1,channel,datapoint):',output.shape)\n",
    "            return output\n",
    "        \n",
    "        data_ndarrays = {}\n",
    "        for key, value in data.items():\n",
    "            # Detect if the key is 'Info' and skip it\n",
    "            if key != 'Info':\n",
    "                data_ndarrays[key] = convert_to_input(value, self.sec_Window, self.sec_Overlap)\n",
    "            else:\n",
    "                data_ndarrays[key] = value\n",
    "\n",
    "        # Label organizing\n",
    "        label_ndarrays = {}\n",
    "        label_ndarrays_CInonCI = {}\n",
    "        for key, value in info.items():\n",
    "            if key != 'Info':\n",
    "                label_ndarrays[key] = value['Label'].values\n",
    "                label_ndarrays_CInonCI[key] = value['Label'].values\n",
    "            else:\n",
    "                label_ndarrays[key] = value\n",
    "                label_ndarrays_CInonCI[key] = value\n",
    "\n",
    "        # Replace 2 with 1 in label_ndarrays\n",
    "        label_ndarrays_CInonCI = {key: np.where(value == 2, 1, value) for key, value in label_ndarrays_CInonCI.items()}\n",
    "\n",
    "        \n",
    "        return data_ndarrays, label_ndarrays, label_ndarrays_CInonCI\n",
    "\n",
    "    def epoch_based_organizing(self, z_score=False, minmax=False):\n",
    "        [data_ndarrays, label_ndarrays, label_ndarrays_CInonCI] = self.data_preprocessing(self.data, self.info, self.sec_Window, self.sec_Overlap)\n",
    "        \n",
    "        # In python, assigning a dict() to variable is just creating a \"reference\".\n",
    "        # Use copy.deepcopy to prevent this situation.\n",
    "        self.data_ndarrays_S = copy.deepcopy(data_ndarrays)\n",
    "        self.label_ndarrays_CInonCI_S = copy.deepcopy(label_ndarrays_CInonCI)\n",
    "\n",
    "        # epoch based Conversion: (subject,epoch,1,channel,datapoint) -> (subject*epoch,1,channel,datapoint)\n",
    "        for key, value in data_ndarrays.items():\n",
    "            [nSubject,nEpoch,one,nChannel,nDatapoint] = value.shape\n",
    "            new_shape = (nSubject*nEpoch, 1, nChannel, nDatapoint)\n",
    "            data_ndarrays[key] = value.reshape(new_shape)\n",
    "            print(f'{key}: (subject*epoch,1,channel,datapoint):',data_ndarrays[key].shape)\n",
    "            if z_score==True:\n",
    "                print(f'z-score normalization of {key}')\n",
    "                data_ndarrays[key] = self.scaling(data_ndarrays[key])\n",
    "                print(f'{key}: (subject*epoch,1,channel,datapoint):',data_ndarrays[key].shape)\n",
    "            elif minmax==True:\n",
    "                print(f'minmax normalization of {key}')\n",
    "                data_ndarrays[key] = self.scaling_minmax(data_ndarrays[key])\n",
    "                print(f'{key}: (subject*epoch,1,channel,datapoint):',data_ndarrays[key].shape)\n",
    "            else:   pass\n",
    "\n",
    "            # Repeate the label based on the nEpoch, i.e. [1,0,1] with nEpoch=3 -> [1,1,1,0,0,0,1,1,1] \n",
    "            label_ndarrays[key] = np.repeat(label_ndarrays[key], nEpoch)\n",
    "            label_ndarrays_CInonCI[key] = np.repeat(label_ndarrays_CInonCI[key], nEpoch)\n",
    "            print(f'epoch based labels of {key}:',label_ndarrays[key].shape)\n",
    "\n",
    "        self.data_ndarrays_E = data_ndarrays\n",
    "        self.label_ndarrays_CInonCI_E = label_ndarrays_CInonCI\n",
    "        return data_ndarrays, label_ndarrays, label_ndarrays_CInonCI\n",
    "\n",
    "\n",
    "    '''\n",
    "    def scaling(self, datx):\n",
    "        from sklearn.preprocessing import scale\n",
    "        # datx: (subject*epoch,1,channel,datapoint)\n",
    "        ndatx = []\n",
    "        # z-score normalization across channels at each time point\n",
    "        if self.at_each_time_point == True:\n",
    "            for dx in datx:\n",
    "                # dx:(1,channel,datapoint)\n",
    "                mean, std = np.average(dx, axis=1), np.std(dx, axis=1)\n",
    "                ndatx.append((dx-mean)/std)\n",
    "        # z-score normalization on each epoch (1,3,500) -> (1500,) then z-score -> reshape back to (1,3,500)\n",
    "        elif self.at_each_time_point == False:\n",
    "            for dx in datx:\n",
    "                ndatx.append(zscore(dx.flatten()).reshape(dx.shape))\n",
    "        return np.array(ndatx)\n",
    "    '''\n",
    "    def scaling(self, datx):\n",
    "        from sklearn.preprocessing import scale\n",
    "        # datx: (subject*epoch,1,channel,datapoint)\n",
    "        ndatx = []\n",
    "        # minmax normalization across channels at each time point\n",
    "        # timepoint-based across channel z_score: TACZS\n",
    "        if self.at_each_time_point == True:\n",
    "            for dx in datx:\n",
    "                # dx:(1,channel,datapoint)\n",
    "                # dxx:(channel, datapoint)\n",
    "                dxx = dx[0,:,:]\n",
    "                ndatx.append( scale(dxx, axis=0).reshape(dx.shape) )\n",
    "            \n",
    "        # minmax normalization on each epoch (1,3,500) -> (1500,) then z-score -> reshape back to (1,3,500)\n",
    "        elif self.at_each_time_point == False:\n",
    "            # epoch-based across channel z_score: EACZS\n",
    "            if self.across_channel == True:\n",
    "                for dx in datx:\n",
    "                    ndatx.append( scale(dx.flatten()).reshape(dx.shape) )\n",
    "            # epoch-based single channel minmax: ESCZS\n",
    "            elif self.across_channel == False:\n",
    "                for dx in datx:\n",
    "                    # dx:(1,channel,datapoint)\n",
    "                    # dxx:(channel, datapoint)\n",
    "                    dxx = dx[0,:,:]\n",
    "                    ndatx.append( scale(dxx, axis=1).reshape(dx.shape) )\n",
    "        return np.array(ndatx)\n",
    "    \n",
    "    def scaling_minmax(self, datx):\n",
    "        from sklearn.preprocessing import minmax_scale\n",
    "        # datx: (subject*epoch,1,channel,datapoint)\n",
    "        ndatx = []\n",
    "        # minmax normalization across channels at each time point\n",
    "        # timepoint-based across channel minmax: TACMM\n",
    "        if self.at_each_time_point == True:\n",
    "            for dx in datx:\n",
    "                # dx:(1,channel,datapoint)\n",
    "                # dxx:(channel, datapoint)\n",
    "                dxx = dx[0,:,:]\n",
    "                ndatx.append( minmax_scale(dxx, axis=0).reshape(dx.shape) )\n",
    "            \n",
    "        # minmax normalization on each epoch (1,3,500) -> (1500,) then z-score -> reshape back to (1,3,500)\n",
    "        elif self.at_each_time_point == False:\n",
    "            # epoch-based across channel minmax: EACMM\n",
    "            if self.across_channel == True:\n",
    "                for dx in datx:\n",
    "                    ndatx.append( minmax_scale(dx.flatten()).reshape(dx.shape) )\n",
    "            # epoch-based single channel minmax: ESCMM\n",
    "            elif self.across_channel == False:\n",
    "                for dx in datx:\n",
    "                    # dx:(1,channel,datapoint)\n",
    "                    # dxx:(channel, datapoint)\n",
    "                    dxx = dx[0,:,:]\n",
    "                    ndatx.append( minmax_scale(dxx, axis=1).reshape(dx.shape) )\n",
    "        return np.array(ndatx)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ''\n",
    "print(a)\n",
    "\n",
    "a += 'a'\n",
    "print(a)\n",
    "\n",
    "a += 'b'\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(0, 60).reshape(2,2,1,3,5)\n",
    "print(data)\n",
    "\n",
    "new_shape = (2*2,1,3,5)\n",
    "\n",
    "data_v = data.reshape(new_shape)\n",
    "print(data_v)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFGPU_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
