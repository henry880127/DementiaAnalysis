{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.io as sio # cannot use for v7.3 mat file\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import os\n",
    "import pickle\n",
    "from EEGNet_function import EEGNet\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% P300 Stack\n",
    "def p300_stack(data, nStack):\n",
    "    # result = np.array()\n",
    "    ind_toDelete = range(nStack)\n",
    "    result = np.mean(data[0:nStack,:,:,:], axis=0, keepdims=True)\n",
    "    data = np.delete(data,ind_toDelete,axis=0)\n",
    "    while data.shape[0] >= nStack:\n",
    "        tmp = np.mean(data[0:nStack,:,:,:], axis=0, keepdims=True)\n",
    "        result = np.vstack([result, tmp])\n",
    "        data = np.delete(data,ind_toDelete,axis=0)\n",
    "        # print(data.shape)\n",
    "    if (data.shape[0] > 0):\n",
    "        tmp = np.mean(data[0:data.shape[0],:,:,:], axis=0, keepdims=True)\n",
    "        result = np.vstack([result, tmp])\n",
    "        ind_toDelete = range(data.shape[0])\n",
    "        data = np.delete(data,ind_toDelete,axis=0)\n",
    "    return result\n",
    "\n",
    "\n",
    "#%% random\n",
    "def random_trials(full_data, output_size, axis_to_choose):\n",
    "    # Create a sample array\n",
    "\n",
    "    # Randomly select an index from each column\n",
    "    random_indices = np.random.choice(full_data.shape[axis_to_choose], output_size, replace=False)\n",
    "    # Use the selected indices to access the data\n",
    "    selected_data = np.take(full_data, indices=random_indices, axis=axis_to_choose)\n",
    "    return selected_data\n",
    "\n",
    "# filtering\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def butter_lowpass_filter(data, lowcut, fs, order):\n",
    "  nyq = fs/2\n",
    "  low = lowcut/nyq\n",
    "  b, a = butter(order, low, btype='low')\n",
    "  y = filtfilt(b, a, data) # zero-phase filter # data: [ch x time]\n",
    "  return y\n",
    "\n",
    "def butter_highpass_filter(data, highcut, fs, order):\n",
    "  nyq = fs/2\n",
    "  high = highcut/nyq\n",
    "  b, a = butter(order, high, btype='high')\n",
    "  y = filtfilt(b, a, data) # zero-phase filter\n",
    "  return y\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order):\n",
    "  nyq = fs/2\n",
    "  low = lowcut/nyq\n",
    "  high = highcut/nyq\n",
    "  b, a = butter(order, [low, high], btype='band')\n",
    "  # demean before filtering\n",
    "  meandat = np.mean(data, axis=1)\n",
    "  data = data - meandat[:, np.newaxis]\n",
    "  y = filtfilt(b, a, data)\n",
    "  return y\n",
    "\n",
    "# re-sample\n",
    "def down_sample(data,ogFs,targetFreq=256):\n",
    "    q = int(np.floor(ogFs/targetFreq))\n",
    "    data = signal.decimate(data,q,axis=1)\n",
    "    return data\n",
    "\n",
    "def loadPickle(pklDir):\n",
    "    with open(pklDir,'rb') as fp:\n",
    "        dd = pickle.load(fp)\n",
    "        return dd\n",
    "    \n",
    "def reshape2Input(a):\n",
    "    newshape = (a.shape[2], 1, a.shape[0], a.shape[1])  # trials , 1 , EEG_channels , sample_points\n",
    "    return a.reshape(newshape)\n",
    "\n",
    "def saveResult(df, save_dir):\n",
    "    df.to_csv(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EEGNet_fit(folder_dir, random_select_trials = False, pkl_name='unname', logs_dir='logs', K=None, startPt=0, epoch_length=1000):\n",
    "    '''data'''\n",
    "    dictData = loadPickle(f\"{folder_dir}/{pkl_name}\")\n",
    "    targetEEG_train = reshape2Input(dictData['targetEEG_train'])[\n",
    "        :, :, :, startPt:startPt+epoch_length]\n",
    "    targetEEG_test = reshape2Input(dictData['targetEEG_test'])[\n",
    "        :, :, :, startPt:startPt+epoch_length]\n",
    "    nontargetEEG_train = reshape2Input(dictData['nontargetEEG_train'])[\n",
    "        :, :, :, startPt:startPt+epoch_length]\n",
    "    nontargetEEG_test = reshape2Input(dictData['nontargetEEG_test'])[\n",
    "        :, :, :, startPt:startPt+epoch_length]\n",
    "    # Decide which of the above data has more trials,\n",
    "    # and randomly choose the trials form larger one.\n",
    "    \n",
    "    # print('targetEEG_train.shape:',targetEEG_train.shape)\n",
    "    # targetEEG_train = p300_stack(targetEEG_train, 5)\n",
    "    # print('targetEEG_train.shape:',targetEEG_train.shape)\n",
    "    # print('targetEEG_test.shape:',targetEEG_test.shape)\n",
    "    # targetEEG_test = p300_stack(targetEEG_test, 5)\n",
    "    # print('targetEEG_test.shape:',targetEEG_test.shape)\n",
    "    # print('nontargetEEG_train.shape:',nontargetEEG_train.shape)\n",
    "    # nontargetEEG_train = p300_stack(nontargetEEG_train, 5)\n",
    "    # print('nontargetEEG_train.shape:',nontargetEEG_train.shape)\n",
    "    # print('nontargetEEG_test.shape:',nontargetEEG_test.shape)\n",
    "    # nontargetEEG_test = p300_stack(nontargetEEG_test, 5)\n",
    "    # print('nontargetEEG_test.shape:',nontargetEEG_test.shape)\n",
    "\n",
    "    if(random_select_trials==True):\n",
    "        if(targetEEG_train.shape[0] > nontargetEEG_train.shape[0]):\n",
    "            targetEEG_train = random_trials(targetEEG_train, nontargetEEG_train.shape[0], 0)\n",
    "        elif(targetEEG_train.shape[0] < nontargetEEG_train.shape[0]):\n",
    "            nontargetEEG_train = random_trials(nontargetEEG_train, targetEEG_train.shape[0], 0)\n",
    "        else:\n",
    "            pass\n",
    "        if(targetEEG_test.shape[0] > nontargetEEG_test.shape[0]):\n",
    "            targetEEG_test = random_trials(targetEEG_test, nontargetEEG_test.shape[0], 0)\n",
    "        elif(targetEEG_test.shape[0] < nontargetEEG_test.shape[0]):\n",
    "            nontargetEEG_test = random_trials(nontargetEEG_test, targetEEG_test.shape[0], 0)\n",
    "        else:\n",
    "            pass\n",
    "    epochs_train = np.concatenate((targetEEG_train, nontargetEEG_train), axis=0)\n",
    "    epochs_test = np.concatenate((targetEEG_test, nontargetEEG_test), axis=0)\n",
    "    \n",
    "    '''Label'''\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    y_train = np.ones((targetEEG_train.shape[0]))\n",
    "    y_train = np.concatenate((y_train, np.ones(nontargetEEG_train.shape[0])+1))\n",
    "    y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test = np.ones((targetEEG_test.shape[0]))\n",
    "    y_test = np.concatenate((y_test, np.ones(nontargetEEG_test.shape[0])+1))\n",
    "    y_test = encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    '''shuffle'''\n",
    "    num_samples = epochs_train.shape[0]\n",
    "    shuffled_indices = np.arange(num_samples)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    epochs_train = epochs_train[shuffled_indices, :, :, :]\n",
    "    y_train = y_train[shuffled_indices,:]\n",
    "    num_samples = epochs_test.shape[0]\n",
    "    shuffled_indices = np.arange(num_samples)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    epochs_test = epochs_test[shuffled_indices, :, :, :]\n",
    "    y_test = y_test[shuffled_indices,:]\n",
    "    print('y_train.shape:', y_train.shape)\n",
    "    print('y_test.shape:', y_test.shape)\n",
    "    print('epochs_train.shape:', epochs_train.shape)\n",
    "    print('epochs_test.shape:', epochs_test.shape)\n",
    "    savingFoldername = 'data_check'\n",
    "    if not os.path.isdir(f'./results/{savingFoldername}'):\n",
    "        os.mkdir(f'./results/{savingFoldername}')\n",
    "    saveResult(pd.DataFrame(y_train), f'./results/{savingFoldername}/y_train.csv')\n",
    "    saveResult(pd.DataFrame(y_test), f'./results/{savingFoldername}/y_test.csv')\n",
    "    \n",
    "    history_list = list()\n",
    "    if (K == None):\n",
    "        pkl_no_ext = \".\".join(pkl_name.split(\".\")[:-1])\n",
    "        # history = model.fit(epochs_train, y_train, epochs_test, y_test)\n",
    "        model = EEGNet(input_shape=(1, epochs_train.shape[2], epochs_train.shape[3]),\n",
    "                   loss='categorical_crossentropy',\n",
    "                   epochs=50,\n",
    "                   batch_size=300,\n",
    "                   kernLength=125,  # Half of sampling rate\n",
    "                   lr=0.0005,\n",
    "                   min_lr=0.0001,\n",
    "                   log_path=f'{logs_dir}/log_{pkl_no_ext}',\n",
    "                   log_path_TB=f'{logs_dir}_TB/log_{pkl_no_ext}_TB',\n",
    "                   model_name=f'EEGNet_{pkl_no_ext}',\n",
    "                   F1=16,\n",
    "                   avgPoolSize_b1=(1, 4),\n",
    "                   avgPoolSize_b2=(1, 8))\n",
    "        history = model.fit(epochs_train, y_train, epochs_test, y_test)\n",
    "        history_list.append(history)\n",
    "        return history_list\n",
    "    else:\n",
    "        dataSet = np.concatenate([epochs_train, epochs_test])\n",
    "        y = np.concatenate([y_train, y_test])\n",
    "        print(f'KFold Applied! dataSet:{dataSet.shape}  y:{y.shape}')\n",
    "        kf = KFold(n_splits=K, shuffle=True, random_state=5)\n",
    "        kf.get_n_splits(dataSet, y)\n",
    "        print(kf)\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(dataSet)):\n",
    "            pkl_no_ext = \".\".join(pkl_name.split(\".\")[:-1])\n",
    "            model = EEGNet(input_shape=(1, epochs_train.shape[2], epochs_train.shape[3]),\n",
    "                   loss='categorical_crossentropy',\n",
    "                   epochs=50,\n",
    "                   batch_size=300,\n",
    "                   kernLength=125,  # Half of sampling rate\n",
    "                   lr=0.0005,\n",
    "                   min_lr=0.0001,\n",
    "                   log_path=f'{logs_dir}/log_{pkl_no_ext}/Fold_{i}',\n",
    "                   log_path_TB=f'{logs_dir}_TB/log_{pkl_no_ext}_TB',\n",
    "                   model_name=f'EEGNet_{pkl_no_ext}_Fold{i}',\n",
    "                   F1=16,\n",
    "                   avgPoolSize_b1=(1, 4),\n",
    "                   avgPoolSize_b2=(1, 8))\n",
    "            epochs_train = dataSet[train_index, :, :, :]\n",
    "            epochs_test = dataSet[test_index, :, :, :]\n",
    "            y_train = y[train_index, :]\n",
    "            y_test = y[test_index, :]\n",
    "            history = model.fit(epochs_train, y_train, epochs_test, y_test)\n",
    "            history_list.append(history)\n",
    "        return history_list\n",
    "\n",
    "def EEGNet_fit_specificCHs(folder_dir, pkl_name='unname', logs_dir='logs', K=None, startPt=0, epoch_length=1000):\n",
    "    '''data'''\n",
    "    ind_CHs = [31]\n",
    "    dictData = loadPickle(f\"{folder_dir}/{pkl_name}\")\n",
    "    targetEEG_train = reshape2Input(dictData['targetEEG_train'])[\n",
    "        :, :, ind_CHs, startPt:startPt+epoch_length]\n",
    "    targetEEG_test = reshape2Input(dictData['targetEEG_test'])[\n",
    "        :, :, ind_CHs, startPt:startPt+epoch_length]\n",
    "    nontargetEEG_train = reshape2Input(dictData['nontargetEEG_train'])[\n",
    "        :, :, ind_CHs, startPt:startPt+epoch_length]\n",
    "    nontargetEEG_test = reshape2Input(dictData['nontargetEEG_test'])[\n",
    "        :, :, ind_CHs, startPt:startPt+epoch_length]\n",
    "    epochs_train = np.concatenate(\n",
    "        (targetEEG_train, nontargetEEG_train), axis=0)\n",
    "    epochs_test = np.concatenate((targetEEG_test, nontargetEEG_test), axis=0)\n",
    "    \n",
    "    '''Label'''\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    y_train = np.ones((targetEEG_train.shape[0]))\n",
    "    y_train = np.concatenate((y_train, np.ones(nontargetEEG_train.shape[0])+1))\n",
    "    y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test = np.ones((targetEEG_test.shape[0]))\n",
    "    y_test = np.concatenate((y_test, np.ones(nontargetEEG_test.shape[0])+1))\n",
    "    y_test = encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    '''shuffle'''\n",
    "    num_samples = epochs_train.shape[0]\n",
    "    shuffled_indices = np.arange(num_samples)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    epochs_train = epochs_train[shuffled_indices, :, :, :]\n",
    "    y_train = y_train[shuffled_indices, :]\n",
    "    num_samples = epochs_test.shape[0]\n",
    "    shuffled_indices = np.arange(num_samples)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    epochs_test = epochs_test[shuffled_indices, :, :, :]\n",
    "    y_test = y_test[shuffled_indices, :]\n",
    "    print('epochs_train.shape:', epochs_train.shape)\n",
    "    print('epochs_test.shape:', epochs_test.shape)\n",
    "    print('y_train:', y_train.shape)\n",
    "    print('y_test.shape:', y_test.shape)\n",
    "    shuffled_indices = np.arange(num_samples)\n",
    "    model = EEGNet(input_shape=(1, epochs_train.shape[2], epochs_train.shape[3]),\n",
    "                   loss='categorical_crossentropy',\n",
    "                   epochs=40,\n",
    "                   batch_size=300,\n",
    "                   kernLength=125,  # Half of sampling rate\n",
    "                   lr=0.0005,\n",
    "                   min_lr=0.0001,\n",
    "                   log_path=f'{logs_dir}/log_{pkl_name}',\n",
    "                   model_name=f'EEGNet_{pkl_name}',\n",
    "                   F1=16,\n",
    "                   avgPoolSize_b1=(1, 8),\n",
    "                   avgPoolSize_b2=(1, 8))\n",
    "    history_list = list()\n",
    "    if (K == None):\n",
    "        history = model.fit(epochs_train, y_train, epochs_test, y_test)\n",
    "        history_list.append(history)\n",
    "        return history_list\n",
    "    else:\n",
    "        dataSet = np.concatenate([epochs_train, epochs_test])\n",
    "        y = np.concatenate([y_train, y_test])\n",
    "        print(f'KFold Applied! dataSet:{dataSet.shape}  y:{y.shape}')\n",
    "        kf = KFold(n_splits=K)\n",
    "        kf.get_n_splits(dataSet, y)\n",
    "        print(kf)\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(dataSet)):\n",
    "            epochs_train = dataSet[train_index, :, :, :]\n",
    "            epochs_test = dataSet[test_index, :, :, :]\n",
    "            y_train = y[train_index, :]\n",
    "            y_test = y[test_index, :]\n",
    "            history = model.fit(epochs_train, y_train, epochs_test, y_test)\n",
    "            history_list.append(history)\n",
    "        return history_list\n",
    "\n",
    "def EEGNet_fit_folder(folder_dir, logs_dir='logs', ch_set='all', K=None, startPt=0,\n",
    "                      epoch_length=1000, savingFoldername='folder', pltfilename='sess', random_select_trials = False):\n",
    "    data_list = os.listdir(folder_dir)[0:2]\n",
    "    data_list = [os.listdir(folder_dir)[0]]\n",
    "    history_list = list()\n",
    "    f, ax = plt.subplots(1, 2, figsize=(15, 15))\n",
    "    ax = ax.ravel()\n",
    "    for i in range(len(data_list)):\n",
    "        pkl = data_list[i]\n",
    "        print(\"current data: \",pkl)\n",
    "        pickle_dir = f'{folder_dir}/{pkl}'\n",
    "        if (ch_set == '20ch'):\n",
    "            history = EEGNet_fit_specificCHs(folder_dir, pkl_name=pkl, logs_dir=logs_dir,\n",
    "                                      startPt=startPt, epoch_length=epoch_length, K=K)\n",
    "        else:\n",
    "            history = EEGNet_fit(folder_dir, random_select_trials=random_select_trials, pkl_name=pkl, logs_dir=logs_dir,\n",
    "                                 startPt=startPt, epoch_length=epoch_length, K=K)\n",
    "        history_list.append(history)\n",
    "\n",
    "        '''Loss Curve Plot'''\n",
    "        for j, fold in enumerate(history):\n",
    "            if (j == 0):\n",
    "                ax[i].plot(fold.history['loss'], '-', color='#1f77b4',\n",
    "                           label='train')  # loss presents the train_loss\n",
    "                ax[i].plot(fold.history['val_loss'], '-',\n",
    "                           color='#ff7f0e', label='validation')\n",
    "            else:\n",
    "                # loss presents the train_loss\n",
    "                ax[i].plot(fold.history['loss'], '-', color='#1f77b4',)\n",
    "                ax[i].plot(fold.history['val_loss'], '-', color='#ff7f0e',)\n",
    "            pkl_no_ext = \".\".join(pkl.split(\".\")[:-1])\n",
    "            ax[i].set_title(pkl_no_ext)\n",
    "            ax[i].set_ylabel('loss')\n",
    "            # ax[i].set_xlim([0,40])\n",
    "            ax[i].set_ylim([0, 1.5])\n",
    "            ax[i].set_yticks([0, 0.5, 1.0, 1.5])\n",
    "            ax[i].set_xlabel('#iter')\n",
    "    ax[-1].legend(bbox_to_anchor=(-5., -0.4, 2, .102),\n",
    "                  loc=0, ncol=3, mode=\"expand\", borderaxespad=0)\n",
    "    plt.tight_layout()\n",
    "    if not os.path.isdir(f'./results/{savingFoldername}'):\n",
    "        os.mkdir(f'./results/{savingFoldername}')\n",
    "    plt.savefig(f'./results/{savingFoldername}/{pltfilename}.png')\n",
    "    return history_list, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current data:  sub01.pkl\n",
      "y_train.shape: (600, 2)\n",
      "y_test.shape: (1680, 2)\n",
      "epochs_train.shape: (600, 1, 32, 150)\n",
      "epochs_test.shape: (1680, 1, 32, 150)\n",
      "KFold Applied! dataSet:(2280, 1, 32, 150)  y:(2280, 2)\n",
      "KFold(n_splits=5, random_state=5, shuffle=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 32, 150)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "b1_Conv2D (Conv2D)              (None, 16, 32, 150)  2000        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "b1_BM1 (BatchNormalization)     (None, 16, 32, 150)  600         b1_Conv2D[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "b1_DeConv2D (DepthwiseConv2D)   (None, 32, 1, 150)   1024        b1_BM1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "b1_BM2 (BatchNormalization)     (None, 32, 1, 150)   600         b1_DeConv2D[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "b1_Act (Activation)             (None, 32, 1, 150)   0           b1_BM2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "b1_AvgPool (AveragePooling2D)   (None, 32, 1, 37)    0           b1_Act[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "b1_Dropout (Dropout)            (None, 32, 1, 37)    0           b1_AvgPool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b2_SeConv2D (SeparableConv2D)   (None, 16, 1, 37)    1504        b1_Dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b2_BM (BatchNormalization)      (None, 16, 1, 37)    148         b2_SeConv2D[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "b2_Act (Activation)             (None, 16, 1, 37)    0           b2_BM[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "b2_AvgPool (AveragePooling2D)   (None, 16, 1, 4)     0           b2_Act[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "b2_Dropout (Dropout)            (None, 16, 1, 4)     0           b2_AvgPool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "query (Dense)                   (None, 16, 1, 8)     40          b2_Dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "key (Dense)                     (None, 16, 1, 8)     40          b2_Dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "value (Dense)                   (None, 16, 1, 8)     40          b2_Dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Attention (MultiHeadAttention)  (None, 16, 1, 8)     2248        query[0][0]                      \n",
      "                                                                 key[0][0]                        \n",
      "                                                                 value[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           Attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_final (Dense)             (None, 2)            258         flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 2)            0           dense_final[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 8,502\n",
      "Trainable params: 7,828\n",
      "Non-trainable params: 674\n",
      "__________________________________________________________________________________________________\n",
      "The first kernel size is (1, 125)\n",
      "Attention has applied\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 3s 44ms/step - loss: 0.6889 - accuracy: 0.5121 - val_loss: 0.6925 - val_accuracy: 0.5175\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69247, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.6834 - accuracy: 0.5345 - val_loss: 0.6915 - val_accuracy: 0.5175\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69247 to 0.69154, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.6679 - accuracy: 0.5806 - val_loss: 0.6893 - val_accuracy: 0.5197\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69154 to 0.68933, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.6395 - accuracy: 0.6557 - val_loss: 0.6845 - val_accuracy: 0.5307\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.68933 to 0.68450, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.5947 - accuracy: 0.7357 - val_loss: 0.6772 - val_accuracy: 0.5285\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.68450 to 0.67718, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.5443 - accuracy: 0.7785 - val_loss: 0.6703 - val_accuracy: 0.5263\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.67718 to 0.67026, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.4754 - accuracy: 0.8218 - val_loss: 0.6663 - val_accuracy: 0.5307\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.67026 to 0.66628, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.3961 - accuracy: 0.8607 - val_loss: 0.6677 - val_accuracy: 0.5351\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.66628\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.3061 - accuracy: 0.8925 - val_loss: 0.6536 - val_accuracy: 0.5373\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.66628 to 0.65356, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2204 - accuracy: 0.9380 - val_loss: 0.6103 - val_accuracy: 0.5702\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.65356 to 0.61035, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1478 - accuracy: 0.9622 - val_loss: 0.5633 - val_accuracy: 0.5965\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.61035 to 0.56331, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0999 - accuracy: 0.9775 - val_loss: 0.5058 - val_accuracy: 0.6184\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.56331 to 0.50576, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0668 - accuracy: 0.9901 - val_loss: 0.4419 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.50576 to 0.44191, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0498 - accuracy: 0.9929 - val_loss: 0.3986 - val_accuracy: 0.7039\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.44191 to 0.39859, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0325 - accuracy: 0.9973 - val_loss: 0.3505 - val_accuracy: 0.7675\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.39859 to 0.35050, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0229 - accuracy: 0.9989 - val_loss: 0.3100 - val_accuracy: 0.8268\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.35050 to 0.30996, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0171 - accuracy: 0.9989 - val_loss: 0.2821 - val_accuracy: 0.8553\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.30996 to 0.28214, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0151 - accuracy: 0.9995 - val_loss: 0.2487 - val_accuracy: 0.8925\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.28214 to 0.24870, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.2216 - val_accuracy: 0.9123\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.24870 to 0.22160, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0115 - accuracy: 0.9995 - val_loss: 0.1988 - val_accuracy: 0.9276\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.22160 to 0.19878, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0101 - accuracy: 0.9989 - val_loss: 0.1729 - val_accuracy: 0.9474\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.19878 to 0.17295, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0082 - accuracy: 0.9995 - val_loss: 0.1505 - val_accuracy: 0.9693\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.17295 to 0.15050, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0086 - accuracy: 0.9995 - val_loss: 0.1355 - val_accuracy: 0.9737\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.15050 to 0.13554, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0080 - accuracy: 0.9995 - val_loss: 0.1245 - val_accuracy: 0.9803\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.13554 to 0.12455, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0066 - accuracy: 0.9995 - val_loss: 0.1065 - val_accuracy: 0.9868\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.12455 to 0.10649, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0882 - val_accuracy: 0.9912\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.10649 to 0.08818, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0758 - val_accuracy: 0.9934\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.08818 to 0.07582, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0643 - val_accuracy: 0.9978\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.07582 to 0.06426, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9978\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.06426 to 0.05386, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.0460 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05386 to 0.04597, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04597 to 0.03892, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0330 - val_accuracy: 0.9978\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.03892 to 0.03298, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0279 - val_accuracy: 0.9978\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.03298 to 0.02787, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.02787 to 0.02407, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.02407 to 0.02125, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.02125 to 0.01946, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0181 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.01946 to 0.01806, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 0.0165 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.01806 to 0.01647, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0141 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.01647 to 0.01412, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.01412 to 0.01297, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0125 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.01297 to 0.01247, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.01247 to 0.01183, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01183 to 0.01147, saving model to logs/log_sub01/Fold_0\\EEGNet_sub01_Fold0_out_weights.h5\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01147\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01147\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01147\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01147\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01147\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01147\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 0.9956\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01147\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1, 32, 150)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "b1_Conv2D (Conv2D)              (None, 16, 32, 150)  2000        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "b1_BM1 (BatchNormalization)     (None, 16, 32, 150)  600         b1_Conv2D[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "b1_DeConv2D (DepthwiseConv2D)   (None, 32, 1, 150)   1024        b1_BM1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "b1_BM2 (BatchNormalization)     (None, 32, 1, 150)   600         b1_DeConv2D[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "b1_Act (Activation)             (None, 32, 1, 150)   0           b1_BM2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "b1_AvgPool (AveragePooling2D)   (None, 32, 1, 37)    0           b1_Act[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "b1_Dropout (Dropout)            (None, 32, 1, 37)    0           b1_AvgPool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b2_SeConv2D (SeparableConv2D)   (None, 16, 1, 37)    1504        b1_Dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b2_BM (BatchNormalization)      (None, 16, 1, 37)    148         b2_SeConv2D[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "b2_Act (Activation)             (None, 16, 1, 37)    0           b2_BM[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "b2_AvgPool (AveragePooling2D)   (None, 16, 1, 4)     0           b2_Act[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "b2_Dropout (Dropout)            (None, 16, 1, 4)     0           b2_AvgPool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "query (Dense)                   (None, 16, 1, 8)     40          b2_Dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "key (Dense)                     (None, 16, 1, 8)     40          b2_Dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "value (Dense)                   (None, 16, 1, 8)     40          b2_Dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Attention (MultiHeadAttention)  (None, 16, 1, 8)     2248        query[0][0]                      \n",
      "                                                                 key[0][0]                        \n",
      "                                                                 value[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           Attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_final (Dense)             (None, 2)            258         flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 2)            0           dense_final[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 8,502\n",
      "Trainable params: 7,828\n",
      "Non-trainable params: 674\n",
      "__________________________________________________________________________________________________\n",
      "The first kernel size is (1, 125)\n",
      "Attention has applied\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 1s 40ms/step - loss: 0.6842 - accuracy: 0.5367 - val_loss: 0.6896 - val_accuracy: 0.5329\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68957, saving model to logs/log_sub01/Fold_1\\EEGNet_sub01_Fold1_out_weights.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m savingFoldername \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest for 0425 EEGNet tutorial\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 12\u001b[0m history_list, lossCurve \u001b[38;5;241m=\u001b[39m \u001b[43mEEGNet_fit_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartPt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartPt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43msavingFoldername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msavingFoldername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpltfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_select_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m last_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, history \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(history_list):\n",
      "Cell \u001b[1;32mIn[4], line 213\u001b[0m, in \u001b[0;36mEEGNet_fit_folder\u001b[1;34m(folder_dir, logs_dir, ch_set, K, startPt, epoch_length, savingFoldername, pltfilename, random_select_trials)\u001b[0m\n\u001b[0;32m    210\u001b[0m     history \u001b[38;5;241m=\u001b[39m EEGNet_fit_specificCHs(folder_dir, pkl_name\u001b[38;5;241m=\u001b[39mpkl, logs_dir\u001b[38;5;241m=\u001b[39mlogs_dir,\n\u001b[0;32m    211\u001b[0m                               startPt\u001b[38;5;241m=\u001b[39mstartPt, epoch_length\u001b[38;5;241m=\u001b[39mepoch_length, K\u001b[38;5;241m=\u001b[39mK)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mEEGNet_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_select_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_select_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkl_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpkl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mstartPt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartPt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m history_list\u001b[38;5;241m.\u001b[39mappend(history)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Loss Curve Plot'''\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 120\u001b[0m, in \u001b[0;36mEEGNet_fit\u001b[1;34m(folder_dir, random_select_trials, pkl_name, logs_dir, K, startPt, epoch_length)\u001b[0m\n\u001b[0;32m    118\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m y[train_index, :]\n\u001b[0;32m    119\u001b[0m     y_test \u001b[38;5;241m=\u001b[39m y[test_index, :]\n\u001b[1;32m--> 120\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     history_list\u001b[38;5;241m.\u001b[39mappend(history)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history_list\n",
      "File \u001b[1;32md:\\\\DementiaAnalysis\\DementiaAnalysis\\EEGNet with Attention Test\\Example\\EEGNet_function.py:221\u001b[0m, in \u001b[0;36mEEGNet.fit\u001b[1;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention has applied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# if self.class_balancing: # compute_class_weight if class_balancing is True\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m#     self.class_weight = compute_class_weight(y_train)\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m#                     validation_data=(X_val, y_val),\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m#                     callbacks=[tensorboard_callback])  # debug:callbacks'es'\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# debug:callbacks'es'\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32mc:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1229\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1226\u001b[0m   val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1227\u001b[0m   epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[1;32m-> 1229\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1230\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:435\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    433\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m--> 435\u001b[0m   \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1369\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1369\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1421\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_weights(\n\u001b[0;32m   1419\u001b[0m         filepath, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[0;32m   1420\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1421\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1423\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2111\u001b[0m, in \u001b[0;36mModel.save\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \n\u001b[0;32m   2071\u001b[0m \u001b[38;5;124;03mPlease see `tf.keras.models.save_model` or the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m   2109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m-> 2111\u001b[0m \u001b[43msave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2112\u001b[0m \u001b[43m                \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_traces\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py:146\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m  \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[0;32m    138\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, sequential\u001b[38;5;241m.\u001b[39mSequential)):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto the Tensorflow SavedModel format (by setting save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor using `save_weights`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 146\u001b[0m   \u001b[43mhdf5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model_to_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mSharedObjectSavingScope():\n",
      "File \u001b[1;32mc:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py:126\u001b[0m, in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    122\u001b[0m   \u001b[38;5;66;03m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[39;00m\n\u001b[0;32m    123\u001b[0m   \u001b[38;5;66;03m# Keras, to avoid breaking TF.js users.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (include_optimizer \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    125\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model\u001b[38;5;241m.\u001b[39moptimizer, optimizer_v1\u001b[38;5;241m.\u001b[39mTFOptimizer)):\n\u001b[1;32m--> 126\u001b[0m     \u001b[43msave_optimizer_weights_to_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m   f\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py:594\u001b[0m, in \u001b[0;36msave_optimizer_weights_to_hdf5_group\u001b[1;34m(hdf5_group, optimizer)\u001b[0m\n\u001b[0;32m    592\u001b[0m weight_values \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mbatch_get_value(symbolic_weights)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(weight_names, weight_values):\n\u001b[1;32m--> 594\u001b[0m   param_dset \u001b[38;5;241m=\u001b[39m \u001b[43mweights_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m val\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# scalar\u001b[39;00m\n\u001b[0;32m    598\u001b[0m     param_dset[()] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\h5py\\_hl\\group.py:148\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    145\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    146\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 148\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32mc:\\Users\\NESS\\.conda\\envs\\TFGPU_2\\lib\\site-packages\\h5py\\_hl\\dataset.py:137\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[1;32m--> 137\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m    140\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5d.pyx:87\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create dataset (name already exists)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_dir = './organized'\n",
    "data_list = os.listdir(folder_dir)\n",
    "# pkl_name = data_list[0]\n",
    "frame = [0, 150]\n",
    "startPt = frame[0]\n",
    "epoch_length = frame[1]-frame[0]\n",
    "K = 5\n",
    "logs_dir = 'logs'\n",
    "savingFoldername = 'test for 0425 EEGNet tutorial'\n",
    "filename = 'test'\n",
    "\n",
    "history_list, lossCurve = EEGNet_fit_folder(folder_dir, logs_dir='logs', ch_set='all', K=K, startPt=startPt,epoch_length=epoch_length,\n",
    "                                             savingFoldername=savingFoldername, pltfilename=filename, random_select_trials=True)\n",
    "\n",
    "\n",
    "last_result = list()\n",
    "for i, history in enumerate(history_list):\n",
    "    for j, fold in enumerate(history):\n",
    "        DF = pd.DataFrame(fold.history)\n",
    "        if (j==0):\n",
    "            col = DF.columns.tolist()\n",
    "            init = np.zeros(len(col)).tolist()\n",
    "            avg_fold = pd.Series(init, col)\n",
    "        avg_fold += DF.iloc[-1, :]\n",
    "    avg_fold = avg_fold / (j+1)\n",
    "    last_result.append(avg_fold)\n",
    "last_result = pd.DataFrame(last_result, index=None)\n",
    "last_result = last_result.reset_index(drop=True)\n",
    "\n",
    "if not os.path.isdir(f'./results/{savingFoldername}'):\n",
    "    os.mkdir(f'./results/{savingFoldername}')\n",
    "saveResult(last_result, f'./results/{savingFoldername}/{filename}.csv')\n",
    "\n",
    "last_result = list()\n",
    "for i, history in enumerate(history_list):\n",
    "    for j, fold in enumerate(history):\n",
    "        DF = pd.DataFrame(fold.history)\n",
    "        if (j==0):\n",
    "            col = DF.columns.tolist()\n",
    "            init = np.zeros(len(col)).tolist()\n",
    "            avg_fold = pd.Series(init, col)\n",
    "        avg_fold += DF.iloc[30, :]\n",
    "    avg_fold = avg_fold / (j+1)\n",
    "    last_result.append(avg_fold)\n",
    "last_result = pd.DataFrame(last_result, index=None)\n",
    "last_result = last_result.reset_index(drop=True)\n",
    "\n",
    "if not os.path.isdir(f'./results/{savingFoldername}'):\n",
    "    os.mkdir(f'./results/{savingFoldername}')\n",
    "saveResult(last_result, f'./results/{savingFoldername}/ES_{filename}.csv')\n",
    "\n",
    "last_result = list()\n",
    "for i, history in enumerate(history_list):\n",
    "    for j, fold in enumerate(history):\n",
    "        DF = pd.DataFrame(fold.history)\n",
    "        if (j==0):\n",
    "            col = DF.columns.tolist()\n",
    "            init = np.zeros(len(col)).tolist()\n",
    "            avg_fold = pd.Series(init, col)\n",
    "        avg_fold += DF.iloc[0, :]\n",
    "    avg_fold = avg_fold / (j+1)\n",
    "    last_result.append(avg_fold)\n",
    "last_result = pd.DataFrame(last_result, index=None)\n",
    "last_result = last_result.reset_index(drop=True)\n",
    "\n",
    "if not os.path.isdir(f'./results/{savingFoldername}'):\n",
    "    os.mkdir(f'./results/{savingFoldername}')\n",
    "saveResult(last_result, f'./results/{savingFoldername}/1st_Iter_{filename}.csv')\n",
    "plt.show()\n",
    "del lossCurve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFGPU_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
